\section{Method}\label{Sec:Method}
In the following section I will give a brief overview over the theory behind Recurrent Neural Networks in General and Long Short-Term Memory (LSTM) network architecture in particular
\subsection{RNN}
Traditional feed-forward neural networks have shown to be successful in modelling a variety of non-linear input-output relations. However a major shortcoming when it comes to estimate variables that are part of a sequence is the fact that these models are limited to static input-output relations. This means that a set of values for the input variables produces the same output no matter where in the sequence it is located. Regarding many kinds of sequential data such as speech or financial time series, the input-output relation is presumed to change over the course of the sequence. A Recurrent Neural Network offers a method to model these kinds of dynamic relationships. The main idea is to use the output of the hidden layer of observation $t-1$ in some way as input to the same layer in observation $t$. A simplified view of the general RNN architecture unfolded across time can be seen in Figure \ref{fig:RNN}. Here we separated the structure of the model at each time step into the Hidden Layer which contains the recurrent structure and the Output Layer which has no connections across time and produces the final predictions $\tilde{y}$. In an RNN the prediction $\tilde{y}_t$ depends on the inputs at all points in the sequence up to index $t$.
The different types of single layer RNN architectures mainly differ in what the hidden layer $H$ actually consists of and which part of its output is passed to the Output Layer and which to the Hidden Layer in the next time step. The simplest specification is to just use the output $h_t$ of a single non-linearity $f$  as input for both. This architecture is called Simple Recurrent Neural Network or Elman Network. In Figure \ref{fig:elman} the architecture of the hidden layer of this network is visualised with the parameter matrices $W$ and $U$ for the weight of the past hidden layer output and current explanatory variables. This results in the following recursive formal definition of the hidden layer output:
\begin{align*}
h_t = f(Wh_{t-1} + Ux_t)
\end{align*}
For this definition to be complete one needs to specify some initial state of the hidden layer output $h_0$. This state can either be fixed to some reasonable level (e.g. 0) or can be treated as an additional set of parameter over which to train the model.
Note that the dimension of the hidden output $h$ can be chosen independently of the number of input variables. Be $d_h$ and $d_x$ the number of hidden layer outputs and input variables respectively than the total number of trainable parameters is: $n_{par} = d_h * (d_h + d_x)$.

\begin{figure}
  \centering
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../../Plots/RNN\string".png}
  \caption{Simplified RNN Architecture}\label{fig:RNN}
\end{figure}

\begin{figure}
  \centering
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../../Plots/elman\string".png}
  \caption{Hidden Layer of Elman Network}\label{fig:elman}
\end{figure}

\subsection{Vanishing Gradient Problem}
Just as Feed Forward Neural Networks, an RNN is trained using the idea of Backpropagation. Therefore the weights are updated using the derivative of the loss function with respect to each weight. In the following I will assume an additive loss function. Therefore the analysis will be limited to the derivative of the loss function for one observation $l_t = l(y_t, \tilde{y}_t)$. From the previously defined general structure of an RNN network we now that the prediction $\tilde{y}_t$ is a function (represented by the Output Layer in  figure \ref{fig:RNN}) of the hidden layer output $h_t$ at that time point. Therefore the loss can be rewritten as:
\begin{align*}
l_t &= l(y_t, \tilde{y}_t) \\
l_t &= l(y_t, f_{output}(h_t))\\
\end{align*}
To get the derivative of this loss with respect to one of the recurrent weights $W_{ij}$ using the chain rule one gets:
\begin{align*}
\frac{dl_t}{dW_{ij}} &= \frac{l(y_t, f_{output}(h_t))}{df_{output}(h_t)}\frac{df_{output}(h_t)}{dh_t}\frac{dh_t}{W_{ij}}
\end{align*}
The first two terms on the right hand side are independent of the structure of the recurrent / hidden layer and in the following I will therefore concentrate on the derivative of the hidden layer output $\frac{dh_t}{W_{ij}}$.
Entering the definition of the hidden layer of a simple RNN architecture we get the following expression using the chain and product rule:
\begin{align*}
\frac{dh_t}{dW_{ij}} &= \frac{df(Wh_{t-1} + Ux_t)}{dW_{ij}} \\
\frac{dh_t}{dW_{ij}} &= \frac{df(Wh_{t-1})}{dWh_{t-1}}\frac{dW}{dW_{ij}}h_{t-1} +\frac{df(Wh_{t-1})}{dWh_{t-1}} W\frac{dh_{t-1}}{dW_{ij}}
\end{align*}
Due to the right hand term $\frac{dh_{t-1}}{dW_{ij}}$ this again is a recursive formula which we can expand up to $ \frac{dh_{0}}{dW_{ij}} = 0$ and get:
\begin{align*}
\frac{dh_t}{dW_{ij}} &= \sum_{k = 1}^t( \prod_{l = 1}^k \frac{df(Wh_{t-l}}{dWh_{t-l}}) W^{k-1} \frac{dW}{dW_{ij}} h_{t-k}
\end{align*}
This means that the effect of the hidden layer output $k$ time steps in the past on the current loss gradient is multiplied by a factor of $(\prod_{l = 1}^k \frac{df(Wh_{t-l}}{dWh_{t-l}}) W^{k-1}\frac{dW}{dW_{ij}}$. Due to its exponential structure the absolute value of this term  will either decay towards zero or exponentially rise, depending on the structure of the function $f$ as well as the determinant of the matrix $W$. These cases are called the \textit{vanishing} and \textit{exploding gradient problem} respectively. While in practice the first problem could be solved relatively easily by clipping the gradient to a certain maxim value, the problem of a vanishing gradient cannot be solved within the framework of this recurrent architecture. This problem severely limits the ability of simple RNNs to learn long term dependencies across time. While in theory simple RNNs could learn dependencies across arbitrarily large time lags in practise they are limited to influences across just a few time steps. Note that the derivative with regard to one of the parameters of the external variables $U_{ij}$ would a very similar structure and learning these parameters would therefore suffer from the same problems.


\subsection{LSTM}


The LSTM architecture tries to overcome the vanishing gradient problem using a number of gates to control how the error gradient is passed through the network. This leads to a somewhat more complicated architecture of the hidden layer $H$ which I visualised in Figure \ref{fig:lstm}. This architecture is based on two main ideas. The first idea is to separate the Hidden layer output into two parts: The cell state $c_t$ and the hidden layer output $h_t$. The second idea is to use logical gates to manipulate the input and output data. This manipulation can be separated into the \textit{forget} ($f_t$), \textit{input}($i_t$) and \textit{output}($o_t$) modules. The forget and output gates consist of sigmoid layers which return values between 0 and 1. In the case of the forget gate the output values are multiplied by the previous cell state to "forget" certain parts by multiplying them with values close to 0.  The output gate does the same to the hidden layer output of the current time step which is generated by applying some non-linear function $f_o$ to the current cell state. The current cell state is determined by additively updating the past cell state after it has passed through the forget gate using the values returned by the input gate. The input gates values are the result of element wise multiplication of the output of a sigmoid layer and a non-linear layer $f_i$. 
This architecture avoids the vanishing gradient problem in two ways. Firstly the cell state is only updated multiplicatively and additively by the output of layers trained on $h_{t-1}$ and $x_t$. This avoids updating the cell state by repeatedly passing it through a function. The second factor is the use of sigmoid gates which set different parts of the signal to zero at each time step. The combination of these factors avoids the kind of exponential decay observed in the previous section, which enables the network to learn longer dependencies. Since the cell state $c_t$ has the same shape as the hidden layer output, an LSTM has four times as many trainable parameters as a simple RNN architecture with the same shape of $h_t$ and $x_t$.
\begin{figure}
  \centering
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../../Plots/lstm\string".png}
  \caption{Hidden Layer of LSTM Network}\label{fig:lstm}
\end{figure}
