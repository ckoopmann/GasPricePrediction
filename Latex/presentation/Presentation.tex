\documentclass{beamer}
\usepackage{graphicx}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{eso-pic}
\usepackage{mathrsfs}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{booktabs}
% \usepackage{bbm}
\usepackage{cooltooltips}
\usepackage{colordef}
\usepackage{beamerdefs}
\usepackage{lvblisting}
\usepackage{listings}

\pgfdeclareimage[height=2cm]{logobig}{hulogo}
\pgfdeclareimage[height=0.7cm]{logosmall}{hulogo}

\renewcommand{\titlescale}{1.0}
\renewcommand{\titlescale}{1.0}
\renewcommand{\leftcol}{0.6}

\title[Natural gas price forecasting using recurrent neural networks]{Master Thesis:  \textit{Natural gas price forecasting using recurrent neural networks}}
\authora{Christian Koopmann}
\authorb{}
\authorc{}


\def\linka{http://https://www.wiwi.hu-berlin.de/de/professuren/quantitativ/wi}
\def\linkb{}
\def\linkc{}

\institute{School of Business and Economics \\
Chair of Information Systems \\
Humboldt--Universitaet zu Berlin \\}

\hypersetup{pdfpagemode=FullScreen}

\begin{document}
\lstset{language=R}
% 0-1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame[plain]{

\titlepage
}

% No Number on the Outline Slide
\useheadtemplate{%
    \raisebox{-0.75cm}{\parbox{\textwidth}{%
            \footnotesize{\color{isegray}%
                \insertsection\ \leavevmode\leaders\hrule height3.2pt depth-2.8pt\hfill\kern0pt\ }}}
}



% 0-3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Outline}

\tableofcontents{}

}

\section{Motivation}
% 0-2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Motivation}
\begin{itemize}
\item \textit{Methodology:} Evaluate the value of Long Short Term Memory Recurrent Neural Networks for time series prediction \begin{itemize}
\item Compare performance to simple RNN, Feed Forward Neural Networks (FFNN), and linear reference models
\end{itemize}
\item \textit{Application:} Support natural gas traders in choosing the optimal trading strategy \begin{itemize}
\item Trading as part of industrial procurement to meet physical demand
\item No speculative trading 
\end{itemize}
\end{itemize}
}

\section{Natural Gas Market}
\frame{
\frametitle{Description of the target variable / gas prices}
\begin{itemize}
\item Different types of natural gas futures differ in various ways
\item The \textbf{Virtual Trading Point} describes in which part of the European natural gas transport network the gas is delivered
\item The \textbf{Delivery Period} describes during which time frame the gas is delivered at a constant rate
\item The target variable is the future price of gas traded at the \textbf{TTF} VTP for delivery in the \textbf{next calendar month}.
\end{itemize}

}


\section{Methodology}
\frame{
\frametitle{Why RNN ?}
\begin{itemize}
\item FFNN can only learn static input-output mappings
\item For machine learning problems based on sequential data the input-output mapping should be dynamic
\item Examples of sequential data: Text, Speech, Videos, Financial Time Series
\item RNN's are able to learn dependencies of arbitrary length, which does not need to be specified.
\item Main idea: use hidden layer output at one time point as input to the hidden layer at the next input
\end{itemize}
}

\frame{
\frametitle{RNN Layout}
\begin{figure}
\includegraphics[height=0.8\textheight,keepaspectratio]{\string"../../Plots/RNN_recurrent\string".png}
\end{figure}
}


\frame{
\frametitle{RNN Layout}
\begin{figure}
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../../Plots/RNN\string".png}
\end{figure}
}


\frame{
\frametitle{Hidden Layer of Simple RNN}
\begin{figure}
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../../Plots/elman\string".png}
\end{figure}
}

\frame{
\frametitle{Hidden Layer of LSTM}
\begin{figure}
\includegraphics[width=0.8\textwidth,keepaspectratio]{\string"../../Plots/LSTM\string".png}
\end{figure}
}

\section{Input Data}
\frame{
\frametitle{Data Overview}
\begin{itemize}
\item All Data was downloaded from the \textit{Thomson Reuters Eikon} data base.
\item \textbf{Energy Commodity Prices:} gas futures, Brent Oil Futures, Coal Futures, Electricity Base / Peak futures
\item \textbf{Exchange Rates:} EUR/GBP, EUR/USD
\item \textbf{Gas Market Fundamentals:} Storage Levels, Pipeline Flows, National Consumption / Production Data
\item All data as daily values (Closing prices) starting between 2010 - 2014. 
\end{itemize}
}

\section{Training / Tuning Approach}
\frame{
\frametitle{Training / Tuning Approach}
\begin{enumerate}
\item Parameter Tuning of univariate models 
\item Variable Selection of multivariate models 
\item Parameter Tuning of multivariate models 
\item Model evaluation
\end{enumerate}
}

\section{Price Level Prediction}
\frame{
\frametitle{Predicting Price Levels}
\begin{itemize}
\item Predict tomorrows closing price of the TTF Front Month future based on all data available up to the current day
\item The MSE loss function is minimized using stochastic gradient descent
\item Linear Reference Model: AR(1) 
\end{itemize}
}


\frame{
\frametitle{Price Level Prediction Results}
\begin{tiny}
\input{val_eval_cv.tex}
\end{tiny}
}


\section{Binary Prediction}
\frame{
\frametitle{Binary Prediction}
\begin{itemize}
\item \textbf{Problem:} \begin{itemize}
\item Decision Problem: Buy today or at any other day before the end of the month to cover physical demand
\item Once bought, futures can't be sold again (Company Policy, Regulation).
\item Therefore optimal trading strategy can't be derived directly from tomorrows price level
\end{itemize}
\item \textbf{Solution:} \begin{itemize}
\item New binary target variable: \textit{Is today's closing price minimal among all closing prices for the rest of the month ?}
\item Yes - 1, No - 0
\item Loss Function: Binary Cross Entropy
\item Naive reference model: $\tilde{y_i} = \frac{1}{N_{i}}$ with $N_i$ the remaining trading days for this month
\end{itemize}
\end{itemize}
}



\frame{
\frametitle{Binary Prediction Results}
\begin{tiny}
\input{min_eval_cv.tex}
\end{tiny}
}

\section{Conclusions / Outlook / Comments}
\frame{
\frametitle{Conclusions}
\begin{itemize}
\item Among univariate models LSTM outperforms alternative models in both prediction problems
\item Among multivariate problems opposite seems to be the case
\item Univariate LSTM shows best relative performance in Price Level Prediction where it is the only model to significantly outperform the Linear Reference
\item Univariate models seem to be better in Price Level Prediction where the opposite is true for the binary case.
\end{itemize}
}

\frame{
\frametitle{Outlook}
There are several ways for possible extension / improvement
\begin{itemize}
\item Extend parameter tuning to choice of optimizer, activation, length, batch size etc.
\item Extend parameter tuning to include multi-level architectures
\item Use a finer tuning grid for the current parameters
\item Use Cross Validation during parameter tuning
\end{itemize}
}

\frame{
\frametitle{Comments}
\begin{itemize}
\item Model Training / Tuning: Python (Keras, Tensorflow), Result Analysis: R
\item Running models on Amazon Web Services, is easier / cheaper than expected and frees valuable resources on local machine
\item Valuable Resources on LSTMs and implementation in Python: \begin{itemize}
\item  BlogPost \textit{Understanding LSTM Networks}: \href{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}
\item  ML Blog \textit{Machinelearningmastery}: \href{https://machinelearningmastery.com/}{https://machinelearningmastery.com/}
\end{itemize}
\end{itemize}
}

\section{Backup}
\frame{
\frametitle{Simple RNN}
\begin{itemize}
\item Different types of RNN differ in the way they connect the hidden layers between time steps
\item The simplest variant treats the previous output $h_t$ in the same way as the other inputs $x_t$
\item $h_t = f(Wh_{t-1} + Ux_t)$
\end{itemize}
}


\frame{
\frametitle{Vanishing Gradient Problem}
\begin{itemize}
\item Recursive definition of the hidden layer can be expanded: \begin{itemize}
\item $h_t = f(Wh_{t-1} + Ux_t)$
\item $h_t = f(W f(Wh_{t-2} + Ux_{t-1}) + Ux_t)$
\end{itemize}
\item Repeated application of chain rule: \begin{itemize}
\item $\frac{h_t}{dW_{ij}} =\frac{df(Wh_{t})}{dWh_{t}}(\frac{dW}{dW_{ij}}h_{t-1} + W\frac{dh_{t-1}}{dW_{ij}})$
\item $\frac{h_t}{dW_{ij}} = \frac{df(Wh_{t})}{dWh_{t}}(\frac{dW}{dW_{ij}}h_{t-1} + W\frac{df(Wh_{t-1})}{dWh_{t-1}}(\frac{dW}{dW_{ij}}h_{t-2} + W\frac{dh_{t-2}}{dW_{ij}}))$
\item $\frac{dh_t}{dW_{ij}} = \sum_{k = 1}^t( \prod_{l = 1}^k \frac{df(Wh_{t-l})}{dWh_{t-l}}) W^{k-1} \frac{dW}{dW_{ij}} h_{t-k}$
\end{itemize}
\end{itemize}
}

\frame{
\frametitle{Vanishing Gradient Problem}
\begin{itemize}
\item Contribution of $\frac{dh_{t-k}}{dW_{ij}}$ is multiplied by $(\prod_{l = 1}^k \frac{df(Wh_{t-l})}{dWh_{t-l}}) W^{k-1}$ \begin{itemize}
\item Exponential behaviour leads to either \textit{exploding} or \textit{vanishing gradient problem}
\item \textit{Exploding} case can be controlled relatively easily by clipping the gradient
\item No solution of \textit{vanishing gradient problem} in this model set-up
\item RNNs unable to learn long term dependencies
\end{itemize}
\end{itemize}
}

\frame{
\frametitle{LSTM}
\begin{itemize}
\item Long Short Term Memory Networks try to overcome this problem by introducing the cell state $c_t$ \begin{itemize}
\item $c_t$ is manipulated in different \textit{gates} 
\item In each gate $c_t$ is multiplied by or added to the output of a layer of neurons with trained weights
\end{itemize}
\item Formal definition:
\begin{itemize}
\item $h_t =o_t*tanh(c_t)$
\item $c_t = f_t*c_{t-1} + i_t$
\item $o_t = \sigma(W_4h_{t-1} + U_4x_t)$
\item $i_t =  \sigma(W_2h_{t-1} + U_2x_t)*tanh(W_3h_{t-1} + U_3x_t)$
\item $f_t =  \sigma(W_1h_{t-1} + U_1x_t)$
\end{itemize}
\item LSTM does not suffer from \textit{vanishing gradient problem} but has four times as many parameters to train with same input data.
\end{itemize}
}

\frame{
\frametitle{Binary Prediction}
\begin{figure}
\includegraphics[height=0.35\textheight,width=0.7\textwidth]{\string"../../Plots/example_plot_level\string".png}
\end{figure}
\begin{figure}
\includegraphics[height=0.35\textheight,width=0.7\textwidth]{\string"../../Plots/example_plot_binary\string".png}
\end{figure}
}

\frame{
\frametitle{Training / Tuning Approach}
\begin{enumerate}
\item Parameter Tuning of univariate models \begin{itemize}
\item Single Train / Test split
\item Training Data 2010 - 2015 / Test Data: 2016
\item Tuning of: Network Architecture, Dropout, Learning Rate
\end{itemize}
\item Variable Selection of multivariate models \begin{itemize}
\item Use tuned parameters from respective univariate model
\item Forward variable selection based on MSE
\item Same Train / Test split as above
\end{itemize}
\item Parameter Tuning of multivariate models \begin{itemize}
\item Use previously selected input variables
\item Same parameters / data as in univariate case
\end{itemize}
\item Model evaluation
\begin{itemize}
\item Month wise cross validation with testing months selected from 01 - 08/2017
\end{itemize}
\end{enumerate}
}

\frame{
\frametitle{Binary Prediction Univariate parameter tuning}
\begin{tiny}
\input{best_models_min_par_tuning.tex}
\end{tiny}
}

\frame{
\frametitle{Binary Prediction Variable Selection}
\begin{tiny}
\input{best_models_min_var_selection.tex}
\end{tiny}
}

\frame{
\frametitle{Binary Prediction Multivariate parameter tuning}
\begin{tiny}
\input{best_models_multivar_min_par_tuning.tex}
\end{tiny}
}

\frame{
\frametitle{Price Level Prediction Univariate parameter tuning}
\begin{tiny}
\input{best_models_val_par_tuning.tex}
\end{tiny}
}

\frame{
\frametitle{Price Level Prediction Variable Selection}
\begin{tiny}
\input{best_models_val_var_selection.tex}
\end{tiny}
}

\frame{
\frametitle{Price Level Prediction Multivariate parameter tuning}
\begin{tiny}
\input{best_models_multivar_val_par_tuning.tex}
\end{tiny}
}

% No number on outline slide
\useheadtemplate{%
    \raisebox{-0.75cm}{\parbox{\textwidth}{%
            \footnotesize{\color{isegray}%
                \insertsection\ \leavevmode\leaders\hrule height3.2pt depth-2.8pt\hfill\kern0pt\ \thesection-\thepage}}}}
\setcounter{section}{1}


\end{document}
