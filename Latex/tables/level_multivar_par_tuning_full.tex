% latex table generated in R 3.3.2 by xtable 1.8-2 package
% Tue Dec  5 14:00:25 2017
\begin{table}[h!]
\centering
 \begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lrrrr}
  \hline
Model & HiddenNeurons & Dropout & LearningRate & MSE \\ 
  \hline
LSTM &     8 & 0.0000 & 0.0001 & 1.7538 \\ 
  LSTM &    16 & 0.0000 & 0.0001 & 5.6046 \\ 
  LSTM &    32 & 0.0000 & 0.0001 & 2.6656 \\ 
  LSTM &     8 & 0.2500 & 0.0001 & 5.1597 \\ 
  LSTM &    16 & 0.2500 & 0.0001 & 1.3408 \\ 
  LSTM &    32 & 0.2500 & 0.0001 & 2.7449 \\ 
  LSTM &     8 & 0.5000 & 0.0001 & 1.9846 \\ 
  LSTM &    16 & 0.5000 & 0.0001 & 1.2358 \\ 
  LSTM &    32 & 0.5000 & 0.0001 & 3.8306 \\ 
  LSTM &     8 & 0.0000 & 0.0010 & 4.0155 \\ 
  LSTM &    16 & 0.0000 & 0.0010 & 2.4706 \\ 
  LSTM &    32 & 0.0000 & 0.0010 & 2.3230 \\ 
  LSTM &     8 & 0.2500 & 0.0010 & 1.3478 \\ 
  LSTM &    16 & 0.2500 & 0.0010 & 2.8079 \\ 
  LSTM &    32 & 0.2500 & 0.0010 & 4.2684 \\ 
  LSTM &     8 & 0.5000 & 0.0010 & 2.5751 \\ 
  LSTM &    16 & 0.5000 & 0.0010 & 0.8162 \\ 
  LSTM &    32 & 0.5000 & 0.0010 & 4.4029 \\ 
  LSTM &     8 & 0.0000 & 0.0100 & 5.2575 \\ 
  LSTM &    16 & 0.0000 & 0.0100 & 7.0970 \\ 
  LSTM &    32 & 0.0000 & 0.0100 & 5.7429 \\ 
  LSTM &     8 & 0.2500 & 0.0100 & 4.5356 \\ 
  LSTM &    16 & 0.2500 & 0.0100 & 6.2968 \\ 
  LSTM &    32 & 0.2500 & 0.0100 & 19.5982 \\ 
  LSTM &     8 & 0.5000 & 0.0100 & 13.7584 \\ 
  LSTM &    16 & 0.5000 & 0.0100 & 11.0591 \\ 
  LSTM &    32 & 0.5000 & 0.0100 & 6.1368 \\ 
  LSTM &     8 & 0.0000 & 0.1000 & 0.6630 \\ 
  LSTM &    16 & 0.0000 & 0.1000 & 124.1666 \\ 
  LSTM &    32 & 0.0000 & 0.1000 & 138.3311 \\ 
  LSTM &     8 & 0.2500 & 0.1000 & 36.0124 \\ 
  LSTM &    16 & 0.2500 & 0.1000 & 73.2290 \\ 
  LSTM &    32 & 0.2500 & 0.1000 & 136.2391 \\ 
  LSTM &     8 & 0.5000 & 0.1000 & 14.7177 \\ 
  LSTM &    16 & 0.5000 & 0.1000 & 40.2793 \\ 
  LSTM &    32 & 0.5000 & 0.1000 & 138.3311 \\ 
  RNN &     8 & 0.0000 & 0.0001 & 2.3958 \\ 
  RNN &    16 & 0.0000 & 0.0001 & 7.1078 \\ 
  RNN &    32 & 0.0000 & 0.0001 & 1.6637 \\ 
  RNN &     8 & 0.2500 & 0.0001 & 0.3209 \\ 
  RNN &    16 & 0.2500 & 0.0001 & 0.2030 \\ 
  RNN &    32 & 0.2500 & 0.0001 & 4.3564 \\ 
  RNN &     8 & 0.5000 & 0.0001 & 20.2858 \\ 
  RNN &    16 & 0.5000 & 0.0001 & 0.1839 \\ 
  RNN &    32 & 0.5000 & 0.0001 & 0.6141 \\ 
  RNN &     8 & 0.0000 & 0.0010 & 0.1450 \\ 
  RNN &    16 & 0.0000 & 0.0010 & 0.2038 \\ 
  RNN &    32 & 0.0000 & 0.0010 & 1.1317 \\ 
  RNN &     8 & 0.2500 & 0.0010 & 0.2175 \\ 
  RNN &    16 & 0.2500 & 0.0010 & 0.1866 \\ 
  RNN &    32 & 0.2500 & 0.0010 & 0.8365 \\ 
  RNN &     8 & 0.5000 & 0.0010 & 0.1316 \\ 
  RNN &    16 & 0.5000 & 0.0010 & 1.3905 \\ 
  RNN &    32 & 0.5000 & 0.0010 & 0.1255 \\ 
  RNN &     8 & 0.0000 & 0.0100 & 7.4133 \\ 
  RNN &    16 & 0.0000 & 0.0100 & 1.7695 \\ 
  RNN &    32 & 0.0000 & 0.0100 & 2.8956 \\ 
  RNN &     8 & 0.2500 & 0.0100 & 0.4069 \\ 
  RNN &    16 & 0.2500 & 0.0100 & 1.6552 \\ 
  RNN &    32 & 0.2500 & 0.0100 & 1.8959 \\ 
  RNN &     8 & 0.5000 & 0.0100 & 3.6124 \\ 
  RNN &    16 & 0.5000 & 0.0100 & 5.0552 \\ 
  RNN &    32 & 0.5000 & 0.0100 & 0.9901 \\ 
  RNN &     8 & 0.0000 & 0.1000 & 71.0199 \\ 
  RNN &    16 & 0.0000 & 0.1000 & 165.8158 \\ 
  RNN &    32 & 0.0000 & 0.1000 & 232.6483 \\ 
  RNN &     8 & 0.2500 & 0.1000 & 22.1804 \\ 
  RNN &    16 & 0.2500 & 0.1000 & 232.6019 \\ 
  RNN &    32 & 0.2500 & 0.1000 & 232.6483 \\ 
  RNN &     8 & 0.5000 & 0.1000 & 59.3420 \\ 
  RNN &    16 & 0.5000 & 0.1000 & 232.3305 \\ 
  RNN &    32 & 0.5000 & 0.1000 & 159.5425 \\ 
  FFNN &     8 & 0.0000 & 0.0001 & 37.5176 \\ 
  FFNN &    16 & 0.0000 & 0.0001 & 37.7026 \\ 
  FFNN &    32 & 0.0000 & 0.0001 & 37.4233 \\ 
  FFNN &     8 & 0.2500 & 0.0001 & 38.9659 \\ 
  FFNN &    16 & 0.2500 & 0.0001 & 39.4065 \\ 
  FFNN &    32 & 0.2500 & 0.0001 & 37.7760 \\ 
  FFNN &     8 & 0.5000 & 0.0001 & 39.2985 \\ 
  FFNN &    16 & 0.5000 & 0.0001 & 38.9632 \\ 
  FFNN &    32 & 0.5000 & 0.0001 & 36.4432 \\ 
  FFNN &     8 & 0.0000 & 0.0010 & 42.1811 \\ 
  FFNN &    16 & 0.0000 & 0.0010 & 31.8851 \\ 
  FFNN &    32 & 0.0000 & 0.0010 & 26.1451 \\ 
  FFNN &     8 & 0.2500 & 0.0010 & 45.7304 \\ 
  FFNN &    16 & 0.2500 & 0.0010 & 38.9218 \\ 
  FFNN &    32 & 0.2500 & 0.0010 & 29.5022 \\ 
  FFNN &     8 & 0.5000 & 0.0010 & 42.5306 \\ 
  FFNN &    16 & 0.5000 & 0.0010 & 38.2763 \\ 
  FFNN &    32 & 0.5000 & 0.0010 & 37.3179 \\ 
  FFNN &     8 & 0.0000 & 0.0100 & 1.5095 \\ 
  FFNN &    16 & 0.0000 & 0.0100 & 1.6073 \\ 
  FFNN &    32 & 0.0000 & 0.0100 & 1.5520 \\ 
  FFNN &     8 & 0.2500 & 0.0100 & 11.9929 \\ 
  FFNN &    16 & 0.2500 & 0.0100 & 12.6686 \\ 
  FFNN &    32 & 0.2500 & 0.0100 & 12.3826 \\ 
  FFNN &     8 & 0.5000 & 0.0100 & 30.8747 \\ 
  FFNN &    16 & 0.5000 & 0.0100 & 30.1352 \\ 
  FFNN &    32 & 0.5000 & 0.0100 & 30.4182 \\ 
  FFNN &     8 & 0.0000 & 0.1000 & 0.8672 \\ 
  FFNN &    16 & 0.0000 & 0.1000 & 1.4035 \\ 
  FFNN &    32 & 0.0000 & 0.1000 & 1.3096 \\ 
  FFNN &     8 & 0.2500 & 0.1000 & 12.0699 \\ 
  FFNN &    16 & 0.2500 & 0.1000 & 11.2422 \\ 
  FFNN &    32 & 0.2500 & 0.1000 & 13.2255 \\ 
  FFNN &     8 & 0.5000 & 0.1000 & 30.7697 \\ 
  FFNN &    16 & 0.5000 & 0.1000 & 26.3228 \\ 
  FFNN &    32 & 0.5000 & 0.1000 & 33.1380 \\ 
  Regression &     0 & 0.0000 & 0.0001 & 17.9537 \\ 
  Regression &     0 & 0.2500 & 0.0001 & 14.1173 \\ 
  Regression &     0 & 0.5000 & 0.0001 & 20.7288 \\ 
  Regression &     0 & 0.0000 & 0.0010 & 2.2049 \\ 
  Regression &     0 & 0.2500 & 0.0010 & 8.9359 \\ 
  Regression &     0 & 0.5000 & 0.0010 & 20.3438 \\ 
  Regression &     0 & 0.0000 & 0.0100 & 1.3602 \\ 
  Regression &     0 & 0.2500 & 0.0100 & 6.2739 \\ 
  Regression &     0 & 0.5000 & 0.0100 & 20.0297 \\ 
  Regression &     0 & 0.0000 & 0.1000 & 1.5269 \\ 
  Regression &     0 & 0.2500 & 0.1000 & 6.8599 \\ 
  Regression &     0 & 0.5000 & 0.1000 & 19.1685 \\ 
   \hline
 \end{tabular}
\end{adjustbox}
\caption{Full Results in multivariate tuning step of price level prediction} 
\label{tab:level.multivar.par.tuning.full}
\end{table}

