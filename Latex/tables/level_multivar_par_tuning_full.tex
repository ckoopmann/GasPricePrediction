% latex table generated in R 3.3.2 by xtable 1.8-2 package
% Wed Dec  6 12:54:27 2017
\begin{table}[h!]
\centering
 \begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lrrrr}
  \hline
Model & HiddenNeurons & Dropout & LearningRate & MSE \\ 
  \hline
LSTM &     8 & 0.0000 & 0.0001 & 2.8231 \\ 
  LSTM &    16 & 0.0000 & 0.0001 & 1.3359 \\ 
  LSTM &    32 & 0.0000 & 0.0001 & 0.9099 \\ 
  LSTM &     8 & 0.2500 & 0.0001 & 0.9495 \\ 
  LSTM &    16 & 0.2500 & 0.0001 & 1.1010 \\ 
  LSTM &    32 & 0.2500 & 0.0001 & 0.7529 \\ 
  LSTM &     8 & 0.5000 & 0.0001 & 2.3481 \\ 
  LSTM &    16 & 0.5000 & 0.0001 & 1.2940 \\ 
  LSTM &    32 & 0.5000 & 0.0001 & 1.3539 \\ 
  LSTM &     8 & 0.0000 & 0.0010 & 0.5838 \\ 
  LSTM &    16 & 0.0000 & 0.0010 & 0.6126 \\ 
  LSTM &    32 & 0.0000 & 0.0010 & 0.8896 \\ 
  LSTM &     8 & 0.2500 & 0.0010 & 0.1976 \\ 
  LSTM &    16 & 0.2500 & 0.0010 & 0.1358 \\ 
  LSTM &    32 & 0.2500 & 0.0010 & 0.1416 \\ 
  LSTM &     8 & 0.5000 & 0.0010 & 0.9849 \\ 
  LSTM &    16 & 0.5000 & 0.0010 & 0.1474 \\ 
  LSTM &    32 & 0.5000 & 0.0010 & 0.1580 \\ 
  LSTM &     8 & 0.0000 & 0.0100 & 19.0478 \\ 
  LSTM &    16 & 0.0000 & 0.0100 & 13.6066 \\ 
  LSTM &    32 & 0.0000 & 0.0100 & 30.2962 \\ 
  LSTM &     8 & 0.2500 & 0.0100 & 0.5463 \\ 
  LSTM &    16 & 0.2500 & 0.0100 & 1.4703 \\ 
  LSTM &    32 & 0.2500 & 0.0100 & 7.9280 \\ 
  LSTM &     8 & 0.5000 & 0.0100 & 0.2576 \\ 
  LSTM &    16 & 0.5000 & 0.0100 & 1.0944 \\ 
  LSTM &    32 & 0.5000 & 0.0100 & 3.2431 \\ 
  LSTM &     8 & 0.0000 & 0.1000 & 0.4471 \\ 
  LSTM &    16 & 0.0000 & 0.1000 & 44.9995 \\ 
  LSTM &    32 & 0.0000 & 0.1000 & 232.6483 \\ 
  LSTM &     8 & 0.2500 & 0.1000 & 0.6437 \\ 
  LSTM &    16 & 0.2500 & 0.1000 & 1.5472 \\ 
  LSTM &    32 & 0.2500 & 0.1000 & 46.6017 \\ 
  LSTM &     8 & 0.5000 & 0.1000 & 3.2278 \\ 
  LSTM &    16 & 0.5000 & 0.1000 & 1.2483 \\ 
  LSTM &    32 & 0.5000 & 0.1000 & 1.6526 \\ 
  RNN &     8 & 0.0000 & 0.0001 & 3.3036 \\ 
  RNN &    16 & 0.0000 & 0.0001 & 1.0561 \\ 
  RNN &    32 & 0.0000 & 0.0001 & 0.5501 \\ 
  RNN &     8 & 0.2500 & 0.0001 & 1.5780 \\ 
  RNN &    16 & 0.2500 & 0.0001 & 1.8375 \\ 
  RNN &    32 & 0.2500 & 0.0001 & 2.3817 \\ 
  RNN &     8 & 0.5000 & 0.0001 & 12.3788 \\ 
  RNN &    16 & 0.5000 & 0.0001 & 4.9635 \\ 
  RNN &    32 & 0.5000 & 0.0001 & 2.8063 \\ 
  RNN &     8 & 0.0000 & 0.0010 & 0.7295 \\ 
  RNN &    16 & 0.0000 & 0.0010 & 0.2911 \\ 
  RNN &    32 & 0.0000 & 0.0010 & 0.7369 \\ 
  RNN &     8 & 0.2500 & 0.0010 & 0.6769 \\ 
  RNN &    16 & 0.2500 & 0.0010 & 0.6633 \\ 
  RNN &    32 & 0.2500 & 0.0010 & 2.3207 \\ 
  RNN &     8 & 0.5000 & 0.0010 & 0.7835 \\ 
  RNN &    16 & 0.5000 & 0.0010 & 0.8751 \\ 
  RNN &    32 & 0.5000 & 0.0010 & 0.3051 \\ 
  RNN &     8 & 0.0000 & 0.0100 & 1.2380 \\ 
  RNN &    16 & 0.0000 & 0.0100 & 0.2943 \\ 
  RNN &    32 & 0.0000 & 0.0100 & 2.6284 \\ 
  RNN &     8 & 0.2500 & 0.0100 & 0.4794 \\ 
  RNN &    16 & 0.2500 & 0.0100 & 0.5413 \\ 
  RNN &    32 & 0.2500 & 0.0100 & 2.2935 \\ 
  RNN &     8 & 0.5000 & 0.0100 & 3.0948 \\ 
  RNN &    16 & 0.5000 & 0.0100 & 2.7584 \\ 
  RNN &    32 & 0.5000 & 0.0100 & 30.3443 \\ 
  RNN &     8 & 0.0000 & 0.1000 & 129.1422 \\ 
  RNN &    16 & 0.0000 & 0.1000 & 138.3204 \\ 
  RNN &    32 & 0.0000 & 0.1000 & 138.3311 \\ 
  RNN &     8 & 0.2500 & 0.1000 & 18.8998 \\ 
  RNN &    16 & 0.2500 & 0.1000 & 64.9254 \\ 
  RNN &    32 & 0.2500 & 0.1000 & 138.3311 \\ 
  RNN &     8 & 0.5000 & 0.1000 & 40.5744 \\ 
  RNN &    16 & 0.5000 & 0.1000 & 55.5439 \\ 
  RNN &    32 & 0.5000 & 0.1000 & 138.3311 \\ 
  FFNN &     8 & 0.0000 & 0.0001 & 38.5083 \\ 
  FFNN &    16 & 0.0000 & 0.0001 & 38.6491 \\ 
  FFNN &    32 & 0.0000 & 0.0001 & 39.9603 \\ 
  FFNN &     8 & 0.2500 & 0.0001 & 38.4613 \\ 
  FFNN &    16 & 0.2500 & 0.0001 & 39.7351 \\ 
  FFNN &    32 & 0.2500 & 0.0001 & 38.4682 \\ 
  FFNN &     8 & 0.5000 & 0.0001 & 40.4587 \\ 
  FFNN &    16 & 0.5000 & 0.0001 & 38.1391 \\ 
  FFNN &    32 & 0.5000 & 0.0001 & 37.4062 \\ 
  FFNN &     8 & 0.0000 & 0.0010 & 35.9651 \\ 
  FFNN &    16 & 0.0000 & 0.0010 & 29.0146 \\ 
  FFNN &    32 & 0.0000 & 0.0010 & 25.0129 \\ 
  FFNN &     8 & 0.2500 & 0.0010 & 40.2300 \\ 
  FFNN &    16 & 0.2500 & 0.0010 & 37.1331 \\ 
  FFNN &    32 & 0.2500 & 0.0010 & 27.2067 \\ 
  FFNN &     8 & 0.5000 & 0.0010 & 43.1140 \\ 
  FFNN &    16 & 0.5000 & 0.0010 & 36.7907 \\ 
  FFNN &    32 & 0.5000 & 0.0010 & 32.2305 \\ 
  FFNN &     8 & 0.0000 & 0.0100 & 1.3280 \\ 
  FFNN &    16 & 0.0000 & 0.0100 & 1.1808 \\ 
  FFNN &    32 & 0.0000 & 0.0100 & 1.1548 \\ 
  FFNN &     8 & 0.2500 & 0.0100 & 11.0429 \\ 
  FFNN &    16 & 0.2500 & 0.0100 & 11.2806 \\ 
  FFNN &    32 & 0.2500 & 0.0100 & 10.8482 \\ 
  FFNN &     8 & 0.5000 & 0.0100 & 25.5989 \\ 
  FFNN &    16 & 0.5000 & 0.0100 & 26.7786 \\ 
  FFNN &    32 & 0.5000 & 0.0100 & 25.3373 \\ 
  FFNN &     8 & 0.0000 & 0.1000 & 1.3645 \\ 
  FFNN &    16 & 0.0000 & 0.1000 & 1.7251 \\ 
  FFNN &    32 & 0.0000 & 0.1000 & 1.3232 \\ 
  FFNN &     8 & 0.2500 & 0.1000 & 10.3974 \\ 
  FFNN &    16 & 0.2500 & 0.1000 & 11.4996 \\ 
  FFNN &    32 & 0.2500 & 0.1000 & 13.3947 \\ 
  FFNN &     8 & 0.5000 & 0.1000 & 28.5052 \\ 
  FFNN &    16 & 0.5000 & 0.1000 & 25.6381 \\ 
  FFNN &    32 & 0.5000 & 0.1000 & 27.4021 \\ 
  Regression &     0 & 0.0000 & 0.0001 & 15.4225 \\ 
  Regression &     0 & 0.2500 & 0.0001 & 15.7721 \\ 
  Regression &     0 & 0.5000 & 0.0001 & 22.3461 \\ 
  Regression &     0 & 0.0000 & 0.0010 & 2.6897 \\ 
  Regression &     0 & 0.2500 & 0.0010 & 8.0724 \\ 
  Regression &     0 & 0.5000 & 0.0010 & 20.2383 \\ 
  Regression &     0 & 0.0000 & 0.0100 & 1.3348 \\ 
  Regression &     0 & 0.2500 & 0.0100 & 6.3661 \\ 
  Regression &     0 & 0.5000 & 0.0100 & 20.4977 \\ 
  Regression &     0 & 0.0000 & 0.1000 & 1.5808 \\ 
  Regression &     0 & 0.2500 & 0.1000 & 6.4418 \\ 
  Regression &     0 & 0.5000 & 0.1000 & 18.7723 \\ 
   \hline
 \end{tabular}
\end{adjustbox}
\caption{Full Results in multivariate tuning step of price level prediction} 
\label{tab:level.multivar.par.tuning.full}
\end{table}

