% latex table generated in R 3.4.0 by xtable 1.8-2 package
% Tue Nov 21 15:29:07 2017
\begin{table}[h!]
\centering
 \begin{adjustbox}{max width=\textwidth}
\begin{tabular}{lrrrr}
  \hline
Model & HiddenNeurons & Dropout & LearningRate & BCE \\ 
  \hline
LSTM &     8 & 0.0000 & 0.0001 & 0.4952 \\ 
  LSTM &    16 & 0.0000 & 0.0001 & 0.4774 \\ 
  LSTM &    32 & 0.0000 & 0.0001 & 0.4671 \\ 
  LSTM &     8 & 0.2500 & 0.0001 & 0.4893 \\ 
  LSTM &    16 & 0.2500 & 0.0001 & 0.4696 \\ 
  LSTM &    32 & 0.2500 & 0.0001 & 0.4744 \\ 
  LSTM &     8 & 0.5000 & 0.0001 & 0.4926 \\ 
  LSTM &    16 & 0.5000 & 0.0001 & 0.4893 \\ 
  LSTM &    32 & 0.5000 & 0.0001 & 0.4809 \\ 
  LSTM &     8 & 0.0000 & 0.0010 & 0.4639 \\ 
  LSTM &    16 & 0.0000 & 0.0010 & 0.7417 \\ 
  LSTM &    32 & 0.0000 & 0.0010 & 0.9400 \\ 
  LSTM &     8 & 0.2500 & 0.0010 & 0.4597 \\ 
  LSTM &    16 & 0.2500 & 0.0010 & 0.4609 \\ 
  LSTM &    32 & 0.2500 & 0.0010 & 0.4494 \\ 
  LSTM &     8 & 0.5000 & 0.0010 & 0.4648 \\ 
  LSTM &    16 & 0.5000 & 0.0010 & 0.4604 \\ 
  LSTM &    32 & 0.5000 & 0.0010 & 0.4546 \\ 
  LSTM &     8 & 0.0000 & 0.0100 & 1.3384 \\ 
  LSTM &    16 & 0.0000 & 0.0100 & 2.8836 \\ 
  LSTM &    32 & 0.0000 & 0.0100 & 1.8183 \\ 
  LSTM &     8 & 0.2500 & 0.0100 & 1.9988 \\ 
  LSTM &    16 & 0.2500 & 0.0100 & 2.0421 \\ 
  LSTM &    32 & 0.2500 & 0.0100 & 1.7978 \\ 
  LSTM &     8 & 0.5000 & 0.0100 & 1.3029 \\ 
  LSTM &    16 & 0.5000 & 0.0100 & 0.4665 \\ 
  LSTM &    32 & 0.5000 & 0.0100 & 0.9719 \\ 
  LSTM &     8 & 0.0000 & 0.1000 &   Inf \\ 
  LSTM &    16 & 0.0000 & 0.1000 & 0.8855 \\ 
  LSTM &    32 & 0.0000 & 0.1000 & 0.8414 \\ 
  LSTM &     8 & 0.2500 & 0.1000 &  \\ 
  LSTM &    16 & 0.2500 & 0.1000 & 0.5769 \\ 
  LSTM &    32 & 0.2500 & 0.1000 & 0.8123 \\ 
  LSTM &     8 & 0.5000 & 0.1000 & 0.4585 \\ 
  LSTM &    16 & 0.5000 & 0.1000 & 0.6707 \\ 
  LSTM &    32 & 0.5000 & 0.1000 & 0.6265 \\ 
  RNN &     8 & 0.0000 & 0.0001 & 0.4857 \\ 
  RNN &    16 & 0.0000 & 0.0001 & 0.5057 \\ 
  RNN &    32 & 0.0000 & 0.0001 & 0.6083 \\ 
  RNN &     8 & 0.2500 & 0.0001 & 0.4924 \\ 
  RNN &    16 & 0.2500 & 0.0001 & 0.4877 \\ 
  RNN &    32 & 0.2500 & 0.0001 & 0.4769 \\ 
  RNN &     8 & 0.5000 & 0.0001 & 0.5316 \\ 
  RNN &    16 & 0.5000 & 0.0001 & 0.5011 \\ 
  RNN &    32 & 0.5000 & 0.0001 & 0.5113 \\ 
  RNN &     8 & 0.0000 & 0.0010 & 0.5008 \\ 
  RNN &    16 & 0.0000 & 0.0010 & 0.7640 \\ 
  RNN &    32 & 0.0000 & 0.0010 & 1.1766 \\ 
  RNN &     8 & 0.2500 & 0.0010 & 0.5469 \\ 
  RNN &    16 & 0.2500 & 0.0010 & 0.5101 \\ 
  RNN &    32 & 0.2500 & 0.0010 & 0.5165 \\ 
  RNN &     8 & 0.5000 & 0.0010 & 0.4877 \\ 
  RNN &    16 & 0.5000 & 0.0010 & 0.5138 \\ 
  RNN &    32 & 0.5000 & 0.0010 & 0.4990 \\ 
  RNN &     8 & 0.0000 & 0.0100 & 0.8469 \\ 
  RNN &    16 & 0.0000 & 0.0100 & 1.1398 \\ 
  RNN &    32 & 0.0000 & 0.0100 & 0.5812 \\ 
  RNN &     8 & 0.2500 & 0.0100 & 0.6140 \\ 
  RNN &    16 & 0.2500 & 0.0100 & 0.5339 \\ 
  RNN &    32 & 0.2500 & 0.0100 & 0.4924 \\ 
  RNN &     8 & 0.5000 & 0.0100 & 0.5180 \\ 
  RNN &    16 & 0.5000 & 0.0100 & 0.4721 \\ 
  RNN &    32 & 0.5000 & 0.0100 & 0.5624 \\ 
  RNN &     8 & 0.0000 & 0.1000 & 0.5515 \\ 
  RNN &    16 & 0.0000 & 0.1000 & 0.9092 \\ 
  RNN &    32 & 0.0000 & 0.1000 & 1.0090 \\ 
  RNN &     8 & 0.2500 & 0.1000 & 0.5612 \\ 
  RNN &    16 & 0.2500 & 0.1000 & 0.5626 \\ 
  RNN &    32 & 0.2500 & 0.1000 & 0.6607 \\ 
  RNN &     8 & 0.5000 & 0.1000 & 0.6073 \\ 
  RNN &    16 & 0.5000 & 0.1000 & 0.7799 \\ 
  RNN &    32 & 0.5000 & 0.1000 & 1.2203 \\ 
  FFNN &     8 & 0.0000 & 0.0001 & 0.6134 \\ 
  FFNN &    16 & 0.0000 & 0.0001 & 0.6093 \\ 
  FFNN &    32 & 0.0000 & 0.0001 & 0.6005 \\ 
  FFNN &     8 & 0.2500 & 0.0001 & 0.6093 \\ 
  FFNN &    16 & 0.2500 & 0.0001 & 0.6109 \\ 
  FFNN &    32 & 0.2500 & 0.0001 & 0.5969 \\ 
  FFNN &     8 & 0.5000 & 0.0001 & 0.6151 \\ 
  FFNN &    16 & 0.5000 & 0.0001 & 0.6086 \\ 
  FFNN &    32 & 0.5000 & 0.0001 & 0.6016 \\ 
  FFNN &     8 & 0.0000 & 0.0010 & 0.5486 \\ 
  FFNN &    16 & 0.0000 & 0.0010 & 0.5478 \\ 
  FFNN &    32 & 0.0000 & 0.0010 & 0.5411 \\ 
  FFNN &     8 & 0.2500 & 0.0010 & 0.5488 \\ 
  FFNN &    16 & 0.2500 & 0.0010 & 0.5477 \\ 
  FFNN &    32 & 0.2500 & 0.0010 & 0.5454 \\ 
  FFNN &     8 & 0.5000 & 0.0010 & 0.5480 \\ 
  FFNN &    16 & 0.5000 & 0.0010 & 0.5471 \\ 
  FFNN &    32 & 0.5000 & 0.0010 & 0.5445 \\ 
  FFNN &     8 & 0.0000 & 0.0100 & 0.4734 \\ 
  FFNN &    16 & 0.0000 & 0.0100 & 0.4769 \\ 
  FFNN &    32 & 0.0000 & 0.0100 & 0.4855 \\ 
  FFNN &     8 & 0.2500 & 0.0100 & 0.4995 \\ 
  FFNN &    16 & 0.2500 & 0.0100 & 0.4974 \\ 
  FFNN &    32 & 0.2500 & 0.0100 & 0.4960 \\ 
  FFNN &     8 & 0.5000 & 0.0100 & 0.5320 \\ 
  FFNN &    16 & 0.5000 & 0.0100 & 0.5308 \\ 
  FFNN &    32 & 0.5000 & 0.0100 & 0.5310 \\ 
  FFNN &     8 & 0.0000 & 0.1000 & 0.5222 \\ 
  FFNN &    16 & 0.0000 & 0.1000 & 0.5822 \\ 
  FFNN &    32 & 0.0000 & 0.1000 & 0.5684 \\ 
  FFNN &     8 & 0.2500 & 0.1000 & 0.4964 \\ 
  FFNN &    16 & 0.2500 & 0.1000 & 0.4954 \\ 
  FFNN &    32 & 0.2500 & 0.1000 & 0.4908 \\ 
  FFNN &     8 & 0.5000 & 0.1000 & 0.5223 \\ 
  FFNN &    16 & 0.5000 & 0.1000 & 0.5310 \\ 
  FFNN &    32 & 0.5000 & 0.1000 & 0.5312 \\ 
  Regression &     0 & 0.0000 & 0.0001 & 0.5778 \\ 
  Regression &     0 & 0.2500 & 0.0001 & 0.5703 \\ 
  Regression &     0 & 0.5000 & 0.0001 & 0.5730 \\ 
  Regression &     0 & 0.0000 & 0.0010 & 0.5451 \\ 
  Regression &     0 & 0.2500 & 0.0010 & 0.5374 \\ 
  Regression &     0 & 0.5000 & 0.0010 & 0.5387 \\ 
  Regression &     0 & 0.0000 & 0.0100 & 0.5003 \\ 
  Regression &     0 & 0.2500 & 0.0100 & 0.5143 \\ 
  Regression &     0 & 0.5000 & 0.0100 & 0.5308 \\ 
  Regression &     0 & 0.0000 & 0.1000 & 0.4904 \\ 
  Regression &     0 & 0.2500 & 0.1000 & 0.5074 \\ 
  Regression &     0 & 0.5000 & 0.1000 & 0.5309 \\ 
   \hline
 \end{tabular}
\end{adjustbox}
\caption{Full Results in multivariate tuning step of binary prediction} 
\label{tab:binary.multivar.par.tuning.full}
\end{table}

